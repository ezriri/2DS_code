{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2a06cde-7385-47be-a070-8239bc6cb747",
   "metadata": {},
   "source": [
    "# 4. h5 -> seperate particles -> cleaning -> png images !\n",
    "This is updated code for reading in 2D-S / HVPS h5 files, extracting the particles, cleaning the images (only significant particles present) and save individual particles as png.\n",
    "\n",
    "this is the streamlined code from previous + also using Jaffeux \n",
    "\n",
    "with optional stats about particles also created\n",
    "\n",
    "this is the looped version, to deal with all the days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f87a63-5cc3-43c6-956c-17bd7fc90cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 11:19:20.104490: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "import xesmf as xe\n",
    "import pandas as pd\n",
    "#import def_homebrew as hb ## homemade functions xox\n",
    "from scipy.special import gamma\n",
    "import netCDF4 as nc\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import h5py ####\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from scipy.ndimage import convolve, label\n",
    "from skimage.measure import regionprops, find_contours\n",
    "from scipy.spatial import ConvexHull, distance_matrix\n",
    "from skimage.morphology import remove_small_holes ## remove holes <3\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from skimage import measure\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c4499fe-bdcb-425c-b5ea-b520aa6f54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## path to raw h5 \n",
    "\n",
    "# 2ds !! be careful of channels\n",
    "path_h5_ds_0 = '/gws/nopw/j04/dcmex/users/ezriab/raw_h5/2ds/ch_0/'\n",
    "path_h5_ds_1 = '/gws/nopw/j04/dcmex/users/ezriab/raw_h5/2ds/ch_1/'\n",
    "\n",
    "# hvps\n",
    "path_h5_hvps = '/gws/nopw/j04/dcmex/users/ezriab/raw_h5/hvps/'\n",
    "\n",
    "base_save_path = '/gws/nopw/j04/dcmex/users/ezriab/'\n",
    "\n",
    "'''\n",
    "## some practice files - don't want to interfear actual data\n",
    "path_h5_ds_0 = '/home/users/esree/data/'\n",
    "path_h5_hvps = '/home/users/esree/data/'\n",
    "\n",
    "#file_name = 'ch_0_ds_Export_base220719154358.h5'\n",
    "#file_name = 'hvps_Export_base220716161816.h5'\n",
    "#file_name = 'Export_base220730153000.h5'\n",
    "\n",
    "#Export_base220730153000.h5\n",
    "'''\n",
    "\n",
    "path = path_h5_ds_0 ######################################################\n",
    "\n",
    "if os.path.exists(path):\n",
    "    # get string of full path + filenames in specif location\n",
    "    file_list = glob(path+'Export_base*.h5') \n",
    "    \n",
    "    # just get file names\n",
    "    file_names = [os.path.basename(file_path) for file_path in file_list]\n",
    "else:\n",
    "    print(\"NOT REAL OH NO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5afe73-92b4-4bed-bce1-1b9f10f17bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting thresholds / res for attaining good particle final images\n",
    "fill_hole_threshold = 5 # max number pixels contained within particle that is filled in\n",
    "\n",
    "minimum_area = 15 # very quick metric to stop the processing of particles with area < 15 pixels\n",
    "\n",
    "length_threshold = 100 #300 # mu - need this minimum length of max dimension to extract the particle\n",
    "pixel_resolution = 10 # mu\n",
    "desired_image_size = 200 # (assume we want a square image) 200 x 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0eeec90-6158-47f5-82f2-1b6f55b209aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Export_base220622110532.h5',\n",
       " 'Export_base220622113222.h5',\n",
       " 'Export_base220622114324.h5',\n",
       " 'Export_base220622114344.h5',\n",
       " 'Export_base220719154358.h5',\n",
       " 'Export_base220720161837.h5',\n",
       " 'Export_base220722154658.h5',\n",
       " 'Export_base220722164909.h5',\n",
       " 'Export_base220723153010.h5',\n",
       " 'Export_base220724153551.h5',\n",
       " 'Export_base220724154137.h5',\n",
       " 'Export_base220725153309.h5',\n",
       " 'Export_base220726150340.h5',\n",
       " 'Export_base220727154103.h5',\n",
       " 'Export_base220727160514.h5',\n",
       " 'Export_base220727160554.h5',\n",
       " 'Export_base220727160906.h5',\n",
       " 'Export_base220729153228.h5',\n",
       " 'Export_base220730153000.h5',\n",
       " 'Export_base220801154558.h5',\n",
       " 'Export_base220802153538.h5',\n",
       " 'Export_base220803153030.h5',\n",
       " 'Export_base220804160810.h5',\n",
       " 'Export_base220806151304.h5',\n",
       " 'Export_base220806153351.h5',\n",
       " 'Export_base220807160624.h5',\n",
       " 'Export_base220807183300.h5',\n",
       " 'Export_base220808160342.h5']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df204e92-e01a-468f-9e51-1719096ec563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220622\n",
      "220622110532\n"
     ]
    }
   ],
   "source": [
    "print(file_names[0][-15:-9])\n",
    "print(file_names[0][-15:-3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883239f-4234-4571-a9d8-7545e77a313c",
   "metadata": {},
   "source": [
    "### save path - will changing for processing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e89cd54-5864-44c6-8ac5-b49c8a0c3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding automation - makes sure to change when processing images !!!!!!!!!!!!!\n",
    "if '2ds' in file_list[0]:\n",
    "    if 'ch_0' in file_list[0]:\n",
    "        save_path = base_save_path+'processed_stats/ch_0/'\n",
    "        particle_type = 'ch_0'\n",
    "\n",
    "    elif 'ch_1' in file_list[0]:\n",
    "        save_path = base_save_path+'processed_stats/ch_1/'\n",
    "        particle_type = 'ch_1'\n",
    "\n",
    "elif 'hvps' in file_list[0]:\n",
    "    save_path = base_save_path+'processed_stats/hvps/'\n",
    "    particle_type = 'hvps'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55dcba01-8231-419b-bd03-c902c429d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gws/nopw/j04/dcmex/users/ezriab/processed_stats/ch_0/\n",
      "ch_0\n"
     ]
    }
   ],
   "source": [
    "print(save_path)\n",
    "print(particle_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f9ac9-ae06-412d-ba40-921319205a73",
   "metadata": {},
   "source": [
    "## je m'appelle open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53119a3a-41e4-4fcc-a386-3de969201550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfile_name = '/home/users/esree/data/Export_base220730153000.h5'\\n# code written to run on 1 h5 file at a time\\nfile_path_name =file_name\\n#file_path_name = path_h5_hvps+example_hvps\\n\\n## open\\nh5_file = h5py.File(file_path_name,'r')\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "file_name = '/home/users/esree/data/Export_base220730153000.h5'\n",
    "# code written to run on 1 h5 file at a time\n",
    "file_path_name =file_name\n",
    "#file_path_name = path_h5_hvps+example_hvps\n",
    "\n",
    "## open\n",
    "h5_file = h5py.File(file_path_name,'r')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eacde50-59f4-4bb5-9054-2be29475a49b",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91ec3d11-a83e-46d6-bc71-46ad90ba1e71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## functions to make code run smoothly\n",
    "def stats_description(bw_crystal, fill_hole_thresh):\n",
    "    '''take binary image, fill in small holes and returns object containing stats about crystal'''\n",
    "    \n",
    "    filled_particle = remove_small_holes(bw_crystal.image, area_threshold=fill_hole_thresh) # fill in voids within binary image - better estimation of stats # may need to be altered\n",
    "    \n",
    "    # can see the filled in particle if needs be\n",
    "    #plt.imshow(filled_particle, cmap='gray')\n",
    "    \n",
    "    if filled_particle.shape[0] < 2 or filled_particle.shape[1] < 2:\n",
    "        return filled_particle, None\n",
    "        \n",
    "    contours = measure.find_contours(filled_particle, 0.5)\n",
    "    if contours:\n",
    "        contour = max(contours, key=lambda x: x.shape[0])  # Sort contours by area (largest first) and select the largest contour\n",
    "        \n",
    "        labeled_image = measure.label(filled_particle)  # Label the image based on the threshold\n",
    "        region = measure.regionprops(labeled_image)[0]  # Assumes largest labeled region corresponds to largest contour\n",
    "        \n",
    "        return filled_particle, region\n",
    "    else:\n",
    "        return filled_particle, None\n",
    "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "## function to calculate truncation of particle\n",
    "def calc_truncation(a_slice, particle):\n",
    "    # the intial slice is the raw 2ds data - of whole array, and particle is one selected by regionprops (and has to be 1s + 0s)\n",
    "    ## assume slices are small + don't contain too many odd bits\n",
    "    # first convert to 0 and 1 for calculation of truncation\n",
    "    alt_crystal = np.where(a_slice == 255, 0, 1) # i.e. 255 (blank area)=0, and where crystal is (was 0) = 1\n",
    "    # so sum up number of particle pixels are on the edge of the slice\n",
    "    first_diode = sum(alt_crystal[0,:]) \n",
    "    last_diode = sum(alt_crystal[-1,:])\n",
    "\n",
    "    ## this calculates how many pixels are top / bottom of the particle + then infer number pixels touching\n",
    "    top_particle = np.sum(particle[0] == 1)\n",
    "    bottom_particle = np.sum(particle[-1] == 1)\n",
    "\n",
    "    n_top, n_bottom = 0, 0  # Initialize variables, default 0 when conditions are not met\n",
    "\n",
    "    # Top pixel touching logic\n",
    "    if first_diode != 0 and first_diode >= top_particle:\n",
    "        n_top = top_particle\n",
    "    elif first_diode == 0:\n",
    "        n_top = 0\n",
    "\n",
    "    # Bottom pixel touching logic\n",
    "    if last_diode != 0 and last_diode >= bottom_particle:\n",
    "        n_bottom = bottom_particle\n",
    "    elif last_diode == 0:\n",
    "        n_bottom = 0\n",
    "\n",
    "    return n_top, n_bottom # number pixels touching top / bottom respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908b4339-8312-411d-8da5-10dce5b8f161",
   "metadata": {},
   "source": [
    "## basic cleaning\n",
    "mask the slices that are < 4 pixels in length (remove lot of tiny particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07c68527-36d4-408c-81cc-fbff113b25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  set up dataframe, used to extract from raw h5 file + has stats about the particle\n",
    "columns = [\n",
    "    \"name\",\n",
    "    \"date\",\n",
    "    \"slice_s_idx\",\n",
    "    \"slice_e_idx\",\n",
    "    \"start_time\",\n",
    "    \"end_time\",\n",
    "    \"d_max\",\n",
    "    \"d_min\",\n",
    "    \"orientation\",\n",
    "    \"centroid\",\n",
    "    \"area\",\n",
    "    \"perimeter\",\n",
    "    \"circularity\",\n",
    "    \"y0\",\n",
    "    \"y1\",\n",
    "    \"probe\",\n",
    "    \"first_diode_trunc\",\n",
    "    \"last_diode_trunc\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f8430-4152-4249-a2b6-f48bf80b21f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flight_220622110532.csv saved sucessfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10534/2380291508.py:132: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  particle_df = pd.concat([particle_df, one_particle_data_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flight_220622113222.csv saved sucessfully!\n",
      "Dataset missing in file: /gws/nopw/j04/dcmex/users/ezriab/raw_h5/2ds/ch_0/Export_base220622114324.h5. Error: \"Unable to synchronously open object (object 'ImageData' doesn't exist)\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10534/2380291508.py:132: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  particle_df = pd.concat([particle_df, one_particle_data_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flight_220622114344.csv saved sucessfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10534/2380291508.py:132: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  particle_df = pd.concat([particle_df, one_particle_data_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#start# outer loop for processing each file ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "for j in range(len(file_list)):\n",
    "#for i in range(3,4):\n",
    "    h5_file = h5py.File(file_list[j],'r')\n",
    "    particle_df = pd.DataFrame(columns=columns) # empty df for each day of flights\n",
    "\n",
    "    try:\n",
    "        h5_image = h5_file['ImageData']\n",
    "        h5_time = h5_file['ImageTimes']\n",
    "    except KeyError as e:\n",
    "        print(f\"Dataset missing in file: {file_list[j]}. Error: {e}\")\n",
    "        continue\n",
    "    \n",
    "    #h5_image = h5_file['ImageData']\n",
    "    #h5_time = h5_file['ImageTimes']\n",
    "    \n",
    "    ##### make xarray of useful time data #####\n",
    "    sec_since = h5_time[:,0]\n",
    "    pixel_slice = h5_time[:,1]\n",
    "    pix_sum = pixel_slice.cumsum(dtype = 'int')\n",
    "    \n",
    "    ## make useful datetime format (not seconds since midnight)\n",
    "    # using the file name for reference\n",
    "    date_str = file_names[j][-15:-9]\n",
    "    long_date_string = file_names[j][-15:-3]\n",
    "    starting_date = datetime.strptime(date_str, '%y%m%d')\n",
    "    time_deltas = [timedelta(seconds=float(sec)) for sec in sec_since]\n",
    "    utc_time = [starting_date + delta for delta in time_deltas]\n",
    "    \n",
    "    time_xr =xr.Dataset({\n",
    "        'utc_time':utc_time,\n",
    "        'pixel_slice': pixel_slice,\n",
    "        'pix_sum': pix_sum})\n",
    "    \n",
    "    ## cleaning of whole h5 file - quick removal of tiny particles\n",
    "    ## this has been edited - allow for corresponding time\n",
    "    pix_sum = time_xr['pix_sum']\n",
    "    utc_time = time_xr['utc_time']\n",
    "    \n",
    "    # Calculate the difference\n",
    "    diff = np.diff(pix_sum.values)\n",
    "    \n",
    "    # Create a mask where the difference is greater than 4 - i.e. select segments of significant sizze\n",
    "    mask = diff > 4\n",
    "    # Apply the mask to select the corresponding values from pix_sum and utc_time\n",
    "    selected_pix_sum = pix_sum[:-1][mask]\n",
    "    selected_utc_time = utc_time[:-1][mask]\n",
    "\n",
    "    # start # inner loop for processing each slice ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ #\n",
    "    for i in range(len(selected_pix_sum)-2):\n",
    "    #for i in range(len(selected_pix_sum)-2):\n",
    "    \n",
    "        # pull out selected area + do analysis\n",
    "        one_crystal = h5_file['ImageData'][:,int(selected_pix_sum[i]):int(selected_pix_sum[i+1])] # extract 1 crystal\n",
    "        \n",
    "        binary_image = (one_crystal == 0) ## important, convert regions where 0 = True (our bits of interest), all else false\n",
    "        \n",
    "        labeled_image, num_features = label(binary_image) # identify connected true areas\n",
    "        # labeled_image = array, with each true area given a number to identify them\n",
    "        # num_features = number of unique connected components in image. Have to literally have adjacent pixel, not diagonal (this will make them seperate)\n",
    "        \n",
    "        props = regionprops(labeled_image) # creates quick list of properties describing each feature detected in the image.\n",
    "        ## (features are measured in ~ pixels)\n",
    "        \n",
    "        if props:\n",
    "            #start # inner loop for processing each particle # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ #\n",
    "            for particle in props:\n",
    "            # go through each particle detected\n",
    "                # quickly get rid of tiny particles\n",
    "                if particle.area >= minimum_area:\n",
    "                    ## basic info\n",
    "                    coords = particle.coords # basically gives coords of each point of interest\n",
    "                    x_values = np.unique(coords[:, 1])\n",
    "                    s_idx = int(selected_pix_sum[i] + x_values[0])\n",
    "                    e_idx = int(selected_pix_sum[i] + x_values[-1])\n",
    "    \n",
    "                    ## more complex stats\n",
    "                    filled_part, spec_region = stats_description(particle,fill_hole_threshold)\n",
    "                    ### !!!!!! important part - this is where useful crystals are getting stats + being recorded in the dataframe\n",
    "                    \n",
    "                    filled_part = filled_part.astype(np.float32) ## convert to float 0 and 1s\n",
    "    \n",
    "                     ## do truncation calc\n",
    "                    first_diode, last_diode = calc_truncation(one_crystal, filled_part)\n",
    "                      \n",
    "                    '''\n",
    "                    ############################# re size ######################################################################\n",
    "                    filled_part = np.expand_dims(filled_part, axis=-1) ## add extra dimention - this is for adding padding\n",
    "                    imagex = tf.image.resize_with_crop_or_pad(filled_part, desired_image_size, desired_image_size)\n",
    "                    \n",
    "                    plt.imshow(imagex, cmap='gray')\n",
    "                    plt.axis('off') # Turn off axis labels\n",
    "                    plt.show()\n",
    "                    ###################################################################################################\n",
    "                    '''\n",
    "                   \n",
    "                    if spec_region and spec_region.major_axis_length * pixel_resolution >= length_threshold:\n",
    "                        ## using circularity calculation from Crosier et al. 2011\n",
    "                        circularity_calc = np.divide((spec_region.perimeter**2),(4*np.pi*spec_region.area))\n",
    "                        \n",
    "                        # nice way of saving data - lenth + measurements are correct in microns\n",
    "                        one_particle_data = {\n",
    "                                #\"image_index\": image_index,\n",
    "                                \"name\": f'{s_idx}_{particle.label}_{particle_type}',\n",
    "                                \"date\" : date_str,\n",
    "                                #\"particle_label\": particle.label,\n",
    "                                \"slice_s_idx\": s_idx,\n",
    "                                \"slice_e_idx\": e_idx,\n",
    "                                \"start_time\": str(selected_utc_time[i].values).split('T')[1], # more friendly time\n",
    "                                \"end_time\": str(selected_utc_time[i+1].values).split('T')[1], # more friendly time\n",
    "                                \n",
    "                                #\"start_time\": str(time_xr['utc_time'][s_idx].values).split('T')[1], # more friendly time\n",
    "                                #\"end_time\": str(time_xr['utc_time'][e_idx].values).split('T')[1],\n",
    "                            \n",
    "                                #\"start_time\": time_xr['utc_time'][s_idx],  # assuming 'time_xr' is pre-defined and syncs with indices\n",
    "                                #\"end_time\": time_xr['utc_time'][e_idx],\n",
    "                                \"d_max\": spec_region.major_axis_length * pixel_resolution, ## d_max\n",
    "                                \"d_min\": spec_region.minor_axis_length * pixel_resolution, ## d_min\n",
    "                                \"orientation\": spec_region.orientation,\n",
    "                                \"centroid\": spec_region.centroid,\n",
    "                                \"area\": (spec_region.area * (pixel_resolution**2)),\n",
    "                                \"perimeter\": (spec_region.perimeter * pixel_resolution),\n",
    "                                \"circularity\": circularity_calc,\n",
    "                                \"y0\": coords[0][0],\n",
    "                                \"y1\": coords[-1][0],\n",
    "                                \"probe\": particle_type,\n",
    "                                \"first_diode_trunc\": first_diode,\n",
    "                                \"last_diode_trunc\": last_diode\n",
    "                                }\n",
    "                        #print(f'{s_idx} done')\n",
    "                        one_particle_data_df = pd.DataFrame([one_particle_data])\n",
    "                        particle_df = pd.concat([particle_df, one_particle_data_df], ignore_index=True)\n",
    "            #end # inner loop for processing each particle # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ #\n",
    "    ## save the stats\n",
    "    if not os.path.exists(f'{save_path}flight_{long_date_string}.csv'):\n",
    "        particle_df.to_csv(f'{save_path}flight_{long_date_string}.csv', index=False) \n",
    "        print(f'flight_{long_date_string}.csv saved sucessfully!')\n",
    "    else:\n",
    "        print(\"file already exists\")\n",
    "    \n",
    "    h5_file.close()    \n",
    "    # end # inner loop for processing each slice ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ #\n",
    "    \n",
    "#end# outer loop for processing each file ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8a1e97-abfd-4f0f-b9e6-d7cfa0459e35",
   "metadata": {},
   "source": [
    "## saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1497a65d-bd7c-468b-a720-55032de7768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is the code for splitting up the big h5 file -> individual images\n",
    "save_path = '/gws/nopw/j04/dcmex/users/ezriab/2dprocessed/'\n",
    "folder_name = f'flight_{file_name[11:23]}' # each flightset -> own folder\n",
    "\n",
    "if not os.path.exists(save_path+folder_name):\n",
    "    os.makedirs(save_path+folder_name)\n",
    "    print(\"Folder created successfully!\")\n",
    "else:\n",
    "    print(\"Folder already exists.\")\n",
    "save_loc = save_path+folder_name+'/'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
