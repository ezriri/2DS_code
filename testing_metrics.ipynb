{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20aaa9d8-a0fd-4894-8c0c-01e512639b9b",
   "metadata": {},
   "source": [
    "# testing metrics\n",
    "this code test different metrics i.e. circularity etc to see if is doing the job it is supposed to.\n",
    "\n",
    "using 30th july processed to run stats\n",
    "\n",
    "also a lot of csv methods used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62d88fb-a74c-49a8-ac1d-73962ef2a67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 11:34:30.446503: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "import xesmf as xe\n",
    "import pandas as pd\n",
    "#import def_homebrew as hb ## homemade functions xox\n",
    "from scipy.special import gamma\n",
    "import netCDF4 as nc\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import h5py ####\n",
    "from PIL import Image\n",
    "#from IPython.display import display #\n",
    "#import cv2 # not working\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from scipy.ndimage import convolve, label\n",
    "from skimage.measure import regionprops, find_contours\n",
    "from scipy.spatial import ConvexHull, distance_matrix\n",
    "from skimage.morphology import remove_small_holes ## remove holes <3\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from skimage import measure\n",
    "from cv2 import cvtColor, COLOR_BGR2GRAY, threshold, THRESH_BINARY, THRESH_OTSU\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c207160-1b96-4e89-97a6-f31a98781804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '/home/users/esree/processed_images/'\n",
    "#\n",
    "\n",
    "## image location ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "image_path = '/gws/nopw/j04/dcmex/users/ezriab/processed_images/2ds/ch_0/v4_2_220730153000/'\n",
    "new_image_loc = '/home/users/esree/image-examples/v4/'\n",
    "## stats location ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#csv_path = '/gws/nopw/j04/dcmex/users/ezriab/processed_images/2ds/ch_0/v3_220730153000/'\n",
    "#csv_path = '/gws/nopw/j04/dcmex/users/ezriab/processed_images/2ds/ch_0/220730153000/'\n",
    "csv_path = '/gws/nopw/j04/dcmex/users/ezriab/processed_stats/hvps/'\n",
    "\n",
    "#csv = 'merged_stats_habits.csv'\n",
    "#csv = '220730153000_stats_habits.csv'\n",
    "#csv= 'v3_220730153000_stats_habits.csv'\n",
    "csv = 'flight_220730152928.csv'\n",
    "\n",
    "og_df = pd.read_csv(csv_path+csv)\n",
    "## again with the new naming scheme in the df, everything is difficult, so are removing it\n",
    "#og_df['name'] = og_df['name'].str.replace('_ch_0', '', regex=False)\n",
    "# remove truncated ones\n",
    "df = og_df[(og_df['first_diode_trunc'] == 0) & (og_df['last_diode_trunc'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cfb48a2-a8e2-45a1-8c86-ea83326ba8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500.199278692033"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diam = og_df['d_max']\n",
    "smallest = min(diam)\n",
    "smallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e222208-60d1-415a-9ba5-e5735a9f68e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "131385_30ch0 copied\n",
      "y\n",
      "135775_30ch0 copied\n",
      "y\n",
      "288472_30ch0 copied\n",
      "y\n",
      "290973_30ch0 copied\n",
      "y\n",
      "315352_30ch0 copied\n",
      "y\n",
      "316364_30ch0 copied\n",
      "y\n",
      "316915_30ch0 copied\n",
      "y\n",
      "318747_30ch0 copied\n",
      "y\n",
      "320298_30ch0 copied\n",
      "y\n",
      "321046_30ch0 copied\n",
      "y\n",
      "325593_30ch0 copied\n",
      "y\n",
      "326080_30ch0 copied\n",
      "y\n",
      "328059_30ch0 copied\n",
      "y\n",
      "337352_30ch0 copied\n",
      "y\n",
      "356856_30ch0 copied\n",
      "y\n",
      "409061_30ch0 copied\n",
      "y\n",
      "421760_30ch0 copied\n",
      "y\n",
      "444419_30ch0 copied\n",
      "y\n",
      "461074_30ch0 copied\n",
      "y\n",
      "462805_30ch0 copied\n",
      "y\n",
      "464692_30ch0 copied\n",
      "y\n",
      "519359_30ch0 copied\n",
      "y\n",
      "763517_30ch0 copied\n",
      "y\n",
      "763674_30ch0 copied\n",
      "y\n",
      "969672_30ch0 copied\n",
      "y\n",
      "998981_30ch0 copied\n",
      "y\n",
      "1002353_30ch0 copied\n",
      "y\n",
      "1031613_30ch0 copied\n"
     ]
    }
   ],
   "source": [
    "trunc_df = og_df[(og_df['first_diode_trunc'] != 0) & (og_df['last_diode_trunc'] != 0)] # both trunc\n",
    "df_or = og_df[(og_df['first_diode_trunc'] != 0) | (og_df['last_diode_trunc'] != 0)]\n",
    "\n",
    "trunc_list = list(trunc_df['name'])\n",
    "\n",
    "#new_image_path = '/gws/nopw/j04/dcmex/users/ezriab/processed_images/2ds/ch_0/220730153000/'\n",
    "save_path = '/home/users/esree/image-examples/v3/trunc/'\n",
    "\n",
    "for name in trunc_list:\n",
    "    try:\n",
    "        og_file = f'{path}{name}.png'\n",
    "        if os.path.exists(og_file):\n",
    "            print('y')\n",
    "            shutil.copy(og_file, save_path)  # Copy the file\n",
    "            print(f'{name} copied')\n",
    "        else:\n",
    "            print(f'{name} doesn\\'t exist')\n",
    "        \n",
    "    except:\n",
    "        print(f'{name} doesn\\'t exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f890b97-0696-451a-94d9-186586aaf7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA 74\n",
      "Co 152\n",
      "CC 409\n",
      "CBC 9\n",
      "CG 1366\n",
      "HPC 1017\n",
      "Dif 447\n",
      "FA 33\n",
      "WD 110\n"
     ]
    }
   ],
   "source": [
    "categories = ['CA', 'Co',  'CC', 'CBC', 'CG', 'HPC', 'Dif', 'FA', 'WD'] # match paper predictions\n",
    "\n",
    "### make dict with each habit broken into a df\n",
    "habit_dict_df = {} # empty dictionary, with each habit as key\n",
    "for habit in categories:\n",
    "    one_habit = df[(df['Category'] == habit)]\n",
    "    habit_dict_df[habit] = one_habit  ## add to dict\n",
    "    habit_count = one_habit['name'].count()\n",
    "    print(f'{habit} {habit_count}')\n",
    "\n",
    "## open specif habit: habit_dict_df['CA'] etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805c13b1-4ac9-450f-9db0-68a47c737d83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### copying png images based on category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f386a7d0-1c73-4ded-8594-a836ed08345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA done!\n",
      "Co done!\n",
      "CC done!\n",
      "CBC done!\n",
      "CG done!\n",
      "HPC done!\n",
      "Dif done!\n",
      "FA done!\n",
      "WD done!\n"
     ]
    }
   ],
   "source": [
    "## number of images to assess \n",
    "n_images = 20\n",
    "\n",
    "## loop for moving particle images to somewhere more local to view + do assessments\n",
    "for habit_class in categories:\n",
    "    random_particles = []\n",
    "    name_list = habit_dict_df[habit_class]['name'].tolist()\n",
    "    new_loc = f'{new_image_loc}{habit_class}/'\n",
    "    if not os.path.exists(new_loc):\n",
    "        os.makedirs(new_loc)\n",
    "    \n",
    "    # Loop to generate random names\n",
    "    while len(random_particles) < n_images:\n",
    "        random_choice = random.choice(name_list)\n",
    "        \n",
    "        if random_choice in habit_dict_df[habit_class]['name'].values:\n",
    "            og_file = f'{image_path}{random_choice}.png'\n",
    "\n",
    "            if os.path.exists(new_loc) and os.path.exists(og_file):\n",
    "                shutil.copy(og_file, new_loc)  # Copy the file\n",
    "                random_particles.append(random_choice)\n",
    "\n",
    "    \n",
    "    print(f'{habit_class} done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315e391-477d-48fe-9482-9d898501b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a needs script further down to be ran before working - !! dic_image_names !!\n",
    "image_names = list(dic_image_names['WD'])\n",
    "new_image_path = '/gws/nopw/j04/dcmex/users/ezriab/processed_images/2ds/ch_0/220730153000/'\n",
    "other_image_loc = '/home/users/esree/image-examples/WD-2/'\n",
    "\n",
    "for name in image_names:\n",
    "    try:\n",
    "        og_file = f'{new_image_path}{name}.png'\n",
    "        if os.path.exists(og_file):\n",
    "            print('y')\n",
    "            shutil.copy(og_file, other_image_loc)  # Copy the file\n",
    "            print(f'{name} copied')\n",
    "        else:\n",
    "            print(f'{name} doesn\\'t exist')\n",
    "        \n",
    "    except:\n",
    "        print(f'{name} doesn\\'t exist')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e204284-557f-41e7-af03-8d1cb44c4f4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### open up stats about particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9538a-a179-4d38-a81f-7aefa7f679b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## list images that have been selected in the folder\n",
    "new_image_loc = '/home/users/esree/image-examples/v2/'\n",
    "categories = ['CA', 'Co',  'CC', 'CBC', 'CG', 'HPC', 'Dif', 'FA', 'WD'] # match paper predictions\n",
    "\n",
    "path_list = []\n",
    "for habit_class in categories:\n",
    "    new_loc = f'{new_image_loc}{habit_class}/'\n",
    "    path_list.append(new_loc)\n",
    "\n",
    "## making dictionary of list of file names in the extracted images\n",
    "dic_image_names = {}\n",
    "for i in range(len(categories)):\n",
    "\n",
    "    if os.path.exists(path_list[i]):\n",
    "        # get string of full path + filenames in specif location\n",
    "        file_list = glob(path_list[i]+'*.png')\n",
    "        \n",
    "        # just get file names\n",
    "        file_names = []\n",
    "        for file_path in file_list:\n",
    "            base_name = os.path.basename(file_path)\n",
    "            name_without_extension = os.path.splitext(base_name)[0] ## basically get rid of .png\n",
    "            file_names.append(name_without_extension)\n",
    "                \n",
    "        dic_image_names[categories[i]] = file_names  ## add to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ad9bd-64bf-4a4c-a951-370c09a88e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a new dictionary from habit_dict_df of just images extracted\n",
    "image_dict_df = {}\n",
    "for habit in categories: #categories:\n",
    "    image_list = dic_image_names[habit]\n",
    "    sub_sample = habit_dict_df[habit][habit_dict_df[habit]['name'].isin(image_list)] # subsample df based on name crieteria\n",
    "    sub_sample.set_index('name', inplace=True) ## quite important, the name is now the index - to pull out \n",
    "    image_dict_df[habit] = sub_sample\n",
    "\n",
    "\n",
    "#habit_dict_df['CA']['name'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe02b4f-c04a-417b-9f64-e1e9371d84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(image_dict_df['CG'].columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f8574-dac1-47b8-8dc4-cdb9082ff967",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(image_dict_df['CG'])\n",
    "#circularity_calc = np.divide((spec_region.perimeter**2),(4*np.pi*spec_region.area))\n",
    "#row = image_dict_df['WD'].loc['37032_4']\n",
    "row = og_df['name'].loc['172078_1']\n",
    "\n",
    "\n",
    "#circularity = np.divide((row['perimeter']**2),(4*np.pi*row['area']))\n",
    "\n",
    "#print(circularity)\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3b189-73c5-407d-8803-ccb8c65b5cbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for row in image_dict_df['WD']:\n",
    "#    print(row)\n",
    "image_dict_df['WD']['circularity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d17c1-0899-432e-96af-58441e69d6fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### subset the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149dcf04-d77b-4c71-885b-0427ac7bc620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['circularity'] = np.floor(df['circularity'] * 10) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900648d-23bc-4f89-899f-2fdb4c40e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['circularity']\n",
    "all_li = df[(df['circularity'] <= 1.2)]\n",
    "li_names = list(all_li['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec39bc-b61b-4cc2-8c48-00db6f935ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image_path = '/gws/nopw/j04/dcmex/users/ezriab/processed_images/2ds/ch_0/220730153000/'\n",
    "save_path = '/home/users/esree/image-examples/v2/WD/actual_circ/'\n",
    "\n",
    "for name in li_names:\n",
    "    try:\n",
    "        og_file = f'{new_image_path}{name}.png'\n",
    "        if os.path.exists(og_file):\n",
    "            print('y')\n",
    "            shutil.copy(og_file, save_path)  # Copy the file\n",
    "            print(f'{name} copied')\n",
    "        else:\n",
    "            print(f'{name} doesn\\'t exist')\n",
    "        \n",
    "    except:\n",
    "        print(f'{name} doesn\\'t exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ebbab-c337-4f40-919d-45d740e3fac3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## okay something is going wrong, need to check h5 files\n",
    "### i am so fucked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873978a1-74a6-4f92-8d97-9470209730c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec82c8a-7587-49a2-9806-021863839c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/gws/nopw/j04/dcmex/users/ezriab/raw_h5/2ds/ch_0/'\n",
    "# e.g. one file \n",
    "file = 'Export_base220730153000.h5'\n",
    "# open file\n",
    "f2ds = h5py.File(path+file,'r')\n",
    "\n",
    "\n",
    "# structure of the file\n",
    "#list(f2ds.keys()) ## ['ImageData', 'ImageTimes']\n",
    "\n",
    "ds_image = f2ds['ImageData'] # shape (128, 200000) --> 128 pixels width, 200000 length\n",
    "ds_time = f2ds['ImageTimes'] # shape (100000, 3) --> (rows, columns) related to time\n",
    "\n",
    "# make time variable -> 3 seperate columns\n",
    "og_t_xr = xr.Dataset({'ImageTimes': (('data', 'time_vars'),ds_time)})\n",
    "sec_since = og_t_xr['ImageTimes'][:,0] # seconds since midnight UTC\n",
    "pixel_slice = og_t_xr['ImageTimes'][:,1] # number of slices of pixel per image (contain -1, to fill)\n",
    "# we can use pixel slice to correcly divide up data ^ the index to call f2ds['ImageData']\n",
    "# these slices have been pre-determined by Jonny + algorithm he has written\n",
    "### basically pixel slice gives a rough idea of particle location\n",
    "\n",
    "bit_time = og_t_xr['ImageTimes'][:,2] # 32 bit (instrument things) - not useful\n",
    "\n",
    "# do cumulative sum of pixel slices -> can use this as index to slice + extract single crystals\n",
    "pix_sum = pixel_slice.cumsum(dim='data', dtype ='int')\n",
    "\n",
    "# shove together into a useful xarray \n",
    "time_xr = xr.Dataset({\n",
    "    'sec_since': sec_since,\n",
    "    'pixel_slice': pixel_slice,\n",
    "    'bit_time': bit_time,\n",
    "    'pix_sum': pix_sum})\n",
    "\n",
    "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "## functions to make code run smoothly\n",
    "def stats_description(bw_crystal, fill_hole_thresh):\n",
    "    #take binary image, fill in small holes and returns object containing stats about crystal'\n",
    "    \n",
    "    filled_particle = remove_small_holes(bw_crystal.image, area_threshold=fill_hole_thresh) # fill in voids within binary image - better estimation of stats # may need to be altered\n",
    "    \n",
    "    # can see the filled in particle if needs be\n",
    "    #plt.imshow(filled_particle, cmap='gray')\n",
    "    \n",
    "    if filled_particle.shape[0] < 2 or filled_particle.shape[1] < 2:\n",
    "        return filled_particle, None\n",
    "        \n",
    "    contours = measure.find_contours(filled_particle, 0.5)\n",
    "    if contours:\n",
    "        contour = max(contours, key=lambda x: x.shape[0])  # Sort contours by area (largest first) and select the largest contour\n",
    "        \n",
    "        labeled_image = measure.label(filled_particle)  # Label the image based on the threshold\n",
    "        region = measure.regionprops(labeled_image)[0]  # Assumes largest labeled region corresponds to largest contour\n",
    "        \n",
    "        return filled_particle, region\n",
    "    else:\n",
    "        return filled_particle, None\n",
    "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "## function to calculate truncation of particle\n",
    "def calc_truncation(a_slice, particle):\n",
    "    # the intial slice is the raw 2ds data - of whole array, and particle is one selected by regionprops (and has to be 1s + 0s)\n",
    "    ## assume slices are small + don't contain too many odd bits\n",
    "    # first convert to 0 and 1 for calculation of truncation\n",
    "    alt_crystal = np.where(a_slice == 255, 0, 1) # i.e. 255 (blank area)=0, and where crystal is (was 0) = 1\n",
    "    # so sum up number of particle pixels are on the edge of the slice\n",
    "    first_diode = sum(alt_crystal[0,:]) \n",
    "    last_diode = sum(alt_crystal[-1,:])\n",
    "\n",
    "    ## this calculates how many pixels are top / bottom of the particle + then infer number pixels touching\n",
    "    top_particle = np.sum(particle[0] == 1)\n",
    "    bottom_particle = np.sum(particle[-1] == 1)\n",
    "\n",
    "    n_top, n_bottom = 0, 0  # Initialize variables, default 0 when conditions are not met\n",
    "\n",
    "    # Top pixel touching logic\n",
    "    if first_diode != 0 and first_diode >= top_particle:\n",
    "        n_top = top_particle\n",
    "    elif first_diode == 0:\n",
    "        n_top = 0\n",
    "\n",
    "    # Bottom pixel touching logic\n",
    "    if last_diode != 0 and last_diode >= bottom_particle:\n",
    "        n_bottom = bottom_particle\n",
    "    elif last_diode == 0:\n",
    "        n_bottom = 0\n",
    "\n",
    "    return n_top, n_bottom # number pixels touching top / bottom respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766cb1d8-de53-4967-8088-8b314c2dc9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e85fba-ff19-41ec-9a8f-f959642e3209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for particle in dic_image_names['WD']:\n",
    "    print(particle)\n",
    "    row = image_dict_df['WD'].loc[particle]\n",
    "    \n",
    "    circularity = np.divide((row['perimeter']**2),(4*np.pi*row['area']))\n",
    "\n",
    "    im_s_idx = row['start_index']\n",
    "    im_e_idx = row['end_index'] ## may want to add more - to look at a bigger slice\n",
    "    \n",
    "    one_crystal = f2ds['ImageData'][:,im_s_idx:im_e_idx] # extract 1 crystal\n",
    "    \n",
    "    ## this will plot the crystal\n",
    "    plt.imshow(one_crystal, cmap='gray')\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c92a8-b6f5-4bd9-adb6-10f71725e8dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "minimum_area = 15 # very quick metric to stop the processing of particles with area < 15 pixels\n",
    "\n",
    "length_threshold = 100 #300 # mu - need this minimum length of max dimension to extract the particle\n",
    "pixel_resolution = 10 # mu\n",
    "desired_image_size = 200 # (assume we want a square image) 200 x 200\n",
    "\n",
    "wd_names = list(dic_image_names['WD'])\n",
    "one_crystal = wd_names[0]\n",
    "print(one_crystal)\n",
    "row = image_dict_df['WD'].loc[one_crystal]\n",
    "    \n",
    "circularity = np.divide((row['perimeter']**2),(4*np.pi*row['area']))\n",
    "\n",
    "im_s_idx = row['start_index']\n",
    "im_e_idx = row['end_index'] ## may want to add more - to look at a bigger slice\n",
    "\n",
    "one_crystal = f2ds['ImageData'][:,im_s_idx:im_e_idx] # extract 1 crystal\n",
    "#one_crystal = f2ds['ImageData'][:,im_s_idx-40:im_e_idx+100] # extract 1 crystal\n",
    "'''\n",
    "## this will plot the crystal\n",
    "plt.imshow(one_crystal, cmap='gray')\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "binary_image = (one_crystal == 0)\n",
    "labeled_image, num_features = label(binary_image)\n",
    "\n",
    "props = regionprops(labeled_image)\n",
    "\n",
    "for particle in props:\n",
    "    filled_part, spec_region = stats_description(particle,5)\n",
    "                    ### !!!!!! important part - this is where useful crystals are getting stats + being recorded in the dataframe\n",
    "                    \n",
    "    filled_part = filled_part.astype(np.float32) ## convert to float 0 and 1s\n",
    "     \n",
    "    if spec_region and spec_region.major_axis_length * pixel_resolution >= length_threshold and spec_region.minor_axis_length > 4 :\n",
    "        print(f'{im_s_idx}_{particle.label}')\n",
    "        plt.imshow(filled_part, cmap='gray')\n",
    "        plt.axis('off')  # Turn off axis labels\n",
    "        plt.show() \n",
    "\n",
    "        filled_part = np.expand_dims(filled_part, axis=-1) ## add extra dimention - this is for adding padding\n",
    "        imagex = tf.image.resize_with_crop_or_pad(filled_part, desired_image_size, desired_image_size)\n",
    "        \n",
    "        plt.imshow(imagex, cmap='gray')\n",
    "        plt.axis('off') # Turn off axis labels\n",
    "        plt.show()\n",
    "\n",
    "        if np.all(imagex == 0):\n",
    "            print(\"The image is blank (all pixels are zero).\")\n",
    "        elif np.all(imagex == imagex[0, 0, 0]):\n",
    "            print(\"The image is blank (all pixels have the same constant value).\")\n",
    "        else:\n",
    "            print(\"The image contains varying pixel values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486aaa1-09d6-4b48-aab0-458aadee4f0f",
   "metadata": {},
   "source": [
    "## okay, updated script testing\n",
    "pull out images with circularity <1.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4aa937-76e6-4dc7-9839-d6da1b215b6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## updated v2 script, pulling out li partilces + saving them\n",
    "stats_csv = '/gws/nopw/j04/dcmex/users/ezriab/processed_images/2ds/ch_0/220730153000/flight_220730153000.csv'\n",
    "new_df = pd.read_csv(stats_csv)\n",
    "\n",
    "# make df of just li < 1.2\n",
    "all_li = new_df[(new_df['circularity'] <= 1.2)]\n",
    "name_list = all_li['name'].apply(lambda x: x[:-5]).tolist() ## get list of names of that satisfy the criteria (and remove last 5 values)\n",
    "name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a1ec7-7875-4531-9671-887562d18e5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_image_path = '/gws/nopw/j04/dcmex/users/ezriab/processed_images/2ds/ch_0/220730153000/'\n",
    "save_path = '/home/users/esree/image-examples/sample-LI/'\n",
    "\n",
    "for name in name_list:\n",
    "    try:\n",
    "        og_file = f'{new_image_path}{name}.png'\n",
    "        if os.path.exists(og_file):\n",
    "            print('y')\n",
    "            shutil.copy(og_file, save_path)  # Copy the file\n",
    "            print(f'{name} copied')\n",
    "        else:\n",
    "            print(f'{name} doesn\\'t exist')\n",
    "        \n",
    "    except:\n",
    "        print(f'{name} doesn\\'t exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a32adab-fb88-4955-a3d7-a08f84961d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
